# Optimization-Algorithms-Of-Deep-Learning
Introduce different optimization algorithms of deep learning.
Apply gradient descent/ SGD/ Adam/ Adagrad/ RMSprop to optimize Beale function.
Compare convergence effect as the number of iterations increases.
