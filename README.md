# Optimization-Algorithms-Of-Deep-Learning
#最优化作业
Introduce different optimization algorithms of deep learning.
Apply gradient descent/ SGD/ Adam/ Adagrad/ RMSprop to optimize Beale function.
Compare convergence effect as the number of iterations increases.
